{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scanner\n",
    "\n",
    "> Scan nbdev notebooks for exported functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import ast\n",
    "import importlib\n",
    "from execnb.nbio import read_nb\n",
    "from fastcore.basics import AttrDict\n",
    "from nbdev.config import get_config\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_export_cells(\n",
    "    nb_path: Path    # Path to the notebook file\n",
    ") -> List[Dict[str, Any]]:  # List of cells with export directives\n",
    "    \"Extract all code cells from a notebook that have export directives\"\n",
    "    nb = read_nb(nb_path)\n",
    "    export_cells = []\n",
    "    \n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'code' and cell.source:\n",
    "            lines = cell.source.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.strip().startswith('#| export'):\n",
    "                    export_cells.append({\n",
    "                        'cell_id': cell.get('id', None),\n",
    "                        'source': cell.source,\n",
    "                        'idx': cell.idx_ if hasattr(cell, 'idx_') else None\n",
    "                    })\n",
    "                    break\n",
    "    \n",
    "    return export_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_definitions(\n",
    "    source: str  # Python source code\n",
    ") -> List[Dict[str, Any]]:  # List of function/class definitions with metadata\n",
    "    \"Extract function and class definitions from source code\"\n",
    "    definitions = []\n",
    "    \n",
    "    # Remove export directive lines\n",
    "    lines = source.split('\\n')\n",
    "    clean_lines = [line for line in lines if not line.strip().startswith('#| ')]\n",
    "    clean_source = '\\n'.join(clean_lines)\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(clean_source)\n",
    "        \n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                # Get the source lines for this definition\n",
    "                start_line = node.lineno - 1\n",
    "                end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line + 1\n",
    "                \n",
    "                def_lines = clean_lines[start_line:end_line]\n",
    "                def_source = '\\n'.join(def_lines)\n",
    "                \n",
    "                definition = {\n",
    "                    'name': node.name,\n",
    "                    'type': type(node).__name__,\n",
    "                    'source': def_source,\n",
    "                    'lineno': node.lineno,\n",
    "                    'is_async': isinstance(node, ast.AsyncFunctionDef)\n",
    "                }\n",
    "                \n",
    "                # For functions, extract parameters\n",
    "                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
    "                    definition['args'] = []\n",
    "                    for arg in node.args.args:\n",
    "                        definition['args'].append({\n",
    "                            'name': arg.arg,\n",
    "                            'annotation': ast.unparse(arg.annotation) if arg.annotation else None\n",
    "                        })\n",
    "                    \n",
    "                    # Add return annotation\n",
    "                    definition['returns'] = ast.unparse(node.returns) if node.returns else None\n",
    "                \n",
    "                definitions.append(definition)\n",
    "                \n",
    "    except SyntaxError as e:\n",
    "        print(f\"Syntax error parsing source: {e}\")\n",
    "        \n",
    "    return definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scan_notebook(\n",
    "    nb_path: Path,  # Path to the notebook to scan\n",
    "    nbs_root: Optional[Path] = None  # Root notebooks directory (for relative paths)\n",
    ") -> List[Dict[str, Any]]:  # List of exported definitions with metadata\n",
    "    \"Scan a notebook and extract all exported function/class definitions\"\n",
    "    export_cells = get_export_cells(nb_path)\n",
    "    all_definitions = []\n",
    "    \n",
    "    for cell in export_cells:\n",
    "        definitions = extract_definitions(cell['source'])\n",
    "        for defn in definitions:\n",
    "            # If nbs_root is provided, use it; otherwise get from config\n",
    "            if nbs_root is None:\n",
    "                cfg = get_config()\n",
    "                nbs_root = Path(cfg.config_path) / cfg.nbs_path\n",
    "            \n",
    "            # Store relative path from nbs directory for nested folders\n",
    "            relative_path = nb_path.relative_to(nbs_root)\n",
    "            \n",
    "            defn['notebook'] = str(relative_path)  # Store full relative path\n",
    "            defn['cell_id'] = cell['cell_id']\n",
    "            \n",
    "            # Try to get the actual function object from exported module\n",
    "            try:\n",
    "                # Get the module name from the notebook name\n",
    "                module_name = nb_path.stem\n",
    "                if module_name.startswith('0'):\n",
    "                    # Handle numbered notebooks like 00_core -> core\n",
    "                    module_name = module_name.split('_', 1)[1] if '_' in module_name else module_name\n",
    "                \n",
    "                # Build module path including subdirectories\n",
    "                module_parts = ['cjm_nbdev_docments']\n",
    "                \n",
    "                # Add subdirectory parts if notebook is in a subdirectory\n",
    "                if relative_path.parent != Path('.'):\n",
    "                    module_parts.extend(relative_path.parent.parts)\n",
    "                \n",
    "                module_parts.append(module_name)\n",
    "                full_module_name = '.'.join(module_parts)\n",
    "                \n",
    "                # Import the module\n",
    "                module = importlib.import_module(full_module_name)\n",
    "                \n",
    "                # Get the function/class object\n",
    "                if hasattr(module, defn['name']):\n",
    "                    defn['func_obj'] = getattr(module, defn['name'])\n",
    "                else:\n",
    "                    defn['func_obj'] = None\n",
    "                    \n",
    "            except Exception:\n",
    "                defn['func_obj'] = None\n",
    "            \n",
    "            all_definitions.append(defn)\n",
    "    \n",
    "    return all_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def scan_project(\n",
    "    nbs_path: Optional[Path] = None,  # Path to notebooks directory (defaults to config.nbs_path)\n",
    "    pattern: str = \"*.ipynb\"  # Pattern for notebook files to scan\n",
    ") -> List[Dict[str, Any]]:  # All exported definitions found in the project\n",
    "    \"Scan all notebooks in a project for exported definitions\"\n",
    "    if nbs_path is None:\n",
    "        cfg = get_config()\n",
    "        nbs_path = Path(cfg.config_path) / cfg.nbs_path\n",
    "    \n",
    "    nbs_path = Path(nbs_path)\n",
    "    all_definitions = []\n",
    "    \n",
    "    # Use rglob to recursively find notebooks in subdirectories\n",
    "    for nb_path in nbs_path.rglob(pattern):\n",
    "        # Skip private notebooks and those in .ipynb_checkpoints\n",
    "        if not nb_path.name.startswith('_') and '.ipynb_checkpoints' not in str(nb_path):\n",
    "            try:\n",
    "                # Pass the nbs_path to scan_notebook so it knows the root\n",
    "                definitions = scan_notebook(nb_path, nbs_root=nbs_path)\n",
    "                all_definitions.extend(definitions)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scanning {nb_path}: {e}\")\n",
    "    \n",
    "    return all_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58 exported definitions\n",
      "- ClassDef: DocmentsCheckResult in 00_core.ipynb\n",
      "- FunctionDef: __post_init__ in 00_core.ipynb\n",
      "- FunctionDef: extract_param_docs_from_func in 00_core.ipynb\n",
      "- FunctionDef: extract_param_docs in 00_core.ipynb\n",
      "- FunctionDef: check_return_doc in 00_core.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Test scanning this project\n",
    "definitions = scan_project()\n",
    "print(f\"Found {len(definitions)} exported definitions\")\n",
    "for defn in definitions[:5]:  # Show first 5\n",
    "    print(f\"- {defn['type']}: {defn['name']} in {defn['notebook']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing nested folder scanning:\n",
      "Created 3 test notebooks:\n",
      "  - actions/button.ipynb\n",
      "  - data_display/table.ipynb\n",
      "  - index.ipynb\n",
      "\n",
      "scan_project found 3 definitions\n",
      "  - test_func in index.ipynb\n",
      "  - test_func in data_display/table.ipynb\n",
      "  - test_func in actions/button.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Test if nested folders would be detected\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a temporary directory structure to test nested scanning\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmp_path = Path(tmpdir)\n",
    "    \n",
    "    # Create nested structure\n",
    "    (tmp_path / \"actions\").mkdir()\n",
    "    (tmp_path / \"data_display\").mkdir()\n",
    "    \n",
    "    # Create dummy notebooks\n",
    "    dummy_nb = {\n",
    "        \"cells\": [\n",
    "            {\"cell_type\": \"code\", \"source\": \"#| export\\ndef test_func(): pass\", \"id\": \"cell-1\"}\n",
    "        ],\n",
    "        \"metadata\": {},\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 5\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    # Create notebooks in root\n",
    "    with open(tmp_path / \"index.ipynb\", \"w\") as f:\n",
    "        json.dump(dummy_nb, f)\n",
    "    \n",
    "    # Create notebooks in subdirectories\n",
    "    with open(tmp_path / \"actions\" / \"button.ipynb\", \"w\") as f:\n",
    "        json.dump(dummy_nb, f)\n",
    "    \n",
    "    with open(tmp_path / \"data_display\" / \"table.ipynb\", \"w\") as f:\n",
    "        json.dump(dummy_nb, f)\n",
    "    \n",
    "    # Test scanning\n",
    "    print(\"Testing nested folder scanning:\")\n",
    "    notebooks = list(tmp_path.rglob(\"*.ipynb\"))\n",
    "    print(f\"Created {len(notebooks)} test notebooks:\")\n",
    "    for nb in sorted(notebooks):\n",
    "        print(f\"  - {nb.relative_to(tmp_path)}\")\n",
    "    \n",
    "    # Test scan_project with nested folders\n",
    "    definitions = scan_project(nbs_path=tmp_path)\n",
    "    print(f\"\\nscan_project found {len(definitions)} definitions\")\n",
    "    for defn in definitions:\n",
    "        print(f\"  - {defn['name']} in {defn['notebook']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in: /mnt/SN850X_8TB_EXT4/Projects/GitHub/cj-mills/cjm-nbdev-docments/nbs\n",
      "Found 12 notebooks (including nested):\n",
      "  - 00_core.ipynb\n",
      "    Export cells: 14\n",
      "  - 01_scanner.ipynb\n",
      "    Export cells: 5\n",
      "  - 02_report.ipynb\n",
      "    Export cells: 8\n",
      "  - 03_autofix.ipynb\n",
      "    Export cells: 29\n",
      "  - 04_cli.ipynb\n",
      "    Export cells: 6\n",
      "  - index.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what notebooks we're finding (including nested folders)\n",
    "from nbdev.config import get_config\n",
    "cfg = get_config()\n",
    "nbs_path = Path(cfg.config_path) / cfg.nbs_path\n",
    "notebooks = list(nbs_path.rglob(\"*.ipynb\"))\n",
    "print(f\"Looking in: {nbs_path}\")\n",
    "print(f\"Found {len(notebooks)} notebooks (including nested):\")\n",
    "for nb in sorted(notebooks):\n",
    "    if not nb.name.startswith('_') and '.ipynb_checkpoints' not in str(nb):\n",
    "        relative_path = nb.relative_to(nbs_path)\n",
    "        print(f\"  - {relative_path}\")\n",
    "        cells = get_export_cells(nb)\n",
    "        if cells:\n",
    "            print(f\"    Export cells: {len(cells)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
